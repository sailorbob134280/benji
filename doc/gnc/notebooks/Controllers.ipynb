{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Controllers\n",
    "\n",
    "This notebook implements and compares three trajectory tracking controllers based on the error dynamics formulation:\n",
    "\n",
    "1. **LQR + Feedforward** — Closed-form optimal linear feedback\n",
    "2. **QP-MPC (OSQP)** — Convex MPC with constraint handling, suitable for embedded\n",
    "3. **Nonlinear MPC** — Full nonlinear dynamics, maximum accuracy\n",
    "\n",
    "All controllers use the decomposition:\n",
    "\n",
    "$$\\mathbf{u} = \\mathbf{u}_{ref} + \\delta\\mathbf{u}$$\n",
    "\n",
    "where $\\mathbf{u}_{ref}$ is the feedforward from the reference trajectory and $\\delta\\mathbf{u}$ is the feedback correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import solve_discrete_are\n",
    "\n",
    "from utils import angle_wrap, state_error, generate_smooth_reference_trajectory\n",
    "from mpc_controller import QPMPC, NonlinearMPC, compute_jacobians_at_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robot parameters\n",
    "L = 0.14        # wheelbase [m]\n",
    "tau = 0.2       # motor time constant [s]\n",
    "v_cruise = 0.25 # cruise speed [m/s]\n",
    "u_max = 0.5     # max wheel command [m/s]\n",
    "\n",
    "# State dimensions\n",
    "n_x, n_u = 5, 2\n",
    "\n",
    "# Simulation parameters\n",
    "T_final = 60.0\n",
    "dt = 0.05       # 20 Hz\n",
    "t = np.arange(0, T_final, dt)\n",
    "N = len(t)\n",
    "\n",
    "# Noise covariances (for realistic simulation)\n",
    "Q_noise = np.diag([1e-5, 1e-5, 1e-6, 1e-4, 1e-4])  # process\n",
    "Q_noise_chol = np.linalg.cholesky(Q_noise)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "print(f\"Robot: L={L*100:.0f}cm, τ={tau}s, v_cruise={v_cruise}m/s\")\n",
    "print(f\"Simulation: {T_final}s at {1/dt:.0f}Hz ({N} steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ms9aiimj6tn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_dynamics(x, u, dt_step, L, tau):\n",
    "    \"\"\"\n",
    "    Propagate state using nonlinear tank drive dynamics.\n",
    "    \n",
    "    State: [x, y, theta, v_l, v_r]\n",
    "    Input: [u_l, u_r] (wheel velocity commands)\n",
    "    \n",
    "    Kinematics:\n",
    "        ẋ = (v_l + v_r)/2 * cos(θ)\n",
    "        ẏ = (v_l + v_r)/2 * sin(θ)\n",
    "        θ̇ = (v_r - v_l) / L\n",
    "    \n",
    "    Motor dynamics:\n",
    "        v̇_l = (u_l - v_l) / τ\n",
    "        v̇_r = (u_r - v_r) / τ\n",
    "    \"\"\"\n",
    "    x_pos, y_pos, theta, v_l, v_r = x\n",
    "    u_l, u_r = u\n",
    "    \n",
    "    v_avg = (v_l + v_r) / 2.0\n",
    "    \n",
    "    x_dot = v_avg * np.cos(theta)\n",
    "    y_dot = v_avg * np.sin(theta)\n",
    "    theta_dot = (v_r - v_l) / L\n",
    "    v_l_dot = (-v_l + u_l) / tau\n",
    "    v_r_dot = (-v_r + u_r) / tau\n",
    "    \n",
    "    return np.array([\n",
    "        x_pos + x_dot * dt_step,\n",
    "        y_pos + y_dot * dt_step,\n",
    "        theta + theta_dot * dt_step,\n",
    "        v_l + v_l_dot * dt_step,\n",
    "        v_r + v_r_dot * dt_step\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trajectory-heading",
   "metadata": {},
   "source": [
    "## Reference Trajectory\n",
    "\n",
    "Generate a waypoint trajectory with turns. See [Reference Trajectories](Reference%20Trajectories.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "waypoints = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [3.0, 3.0],\n",
    "    [6.0, 2.0],\n",
    "    [8.0, 5.0],\n",
    "])\n",
    "\n",
    "x_ref, u_ref = generate_smooth_reference_trajectory(waypoints, t, v_cruise, L, tau, turn_time=1.0)\n",
    "\n",
    "# Quick plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_ref[0, :], x_ref[1, :], 'k--', linewidth=1, alpha=0.7, label='Reference')\n",
    "plt.plot(waypoints[:, 0], waypoints[:, 1], 'ko', markersize=8)\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('Reference Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lqr-heading",
   "metadata": {},
   "source": [
    "## Controller 1: LQR + Feedforward\n",
    "\n",
    "The LQR controller computes an optimal linear feedback gain $K$ that minimizes:\n",
    "\n",
    "$$J = \\sum_{k=0}^{\\infty} \\left( \\mathbf{e}_k^T Q \\mathbf{e}_k + \\delta\\mathbf{u}_k^T R \\delta\\mathbf{u}_k \\right)$$\n",
    "\n",
    "The control law is:\n",
    "\n",
    "$$\\mathbf{u} = \\mathbf{u}_{ref} - K \\mathbf{e}$$\n",
    "\n",
    "where $\\mathbf{e} = \\mathbf{x} - \\mathbf{x}_{ref}$ (with angle wrapping on $\\theta$).\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The gain $K$ is computed by solving the discrete-time algebraic Riccati equation (DARE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqr-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LQR cost weights\n",
    "Q_lqr = np.diag([20.0, 20.0, 5.0, 0.1, 0.1])  # [x, y, θ, v_l, v_r]\n",
    "R_lqr = np.diag([1.0, 1.0])                    # [u_l, u_r]\n",
    "\n",
    "# Linearize at nominal forward motion (θ=0, v=v_cruise)\n",
    "# This is a design choice - the gain won't be optimal everywhere\n",
    "x_nom = np.array([0, 0, 0, v_cruise, v_cruise])\n",
    "A_c, B_c = compute_jacobians_at_state(x_nom, L, tau)\n",
    "\n",
    "# Discretize\n",
    "A_d = np.eye(n_x) + dt * A_c\n",
    "B_d = dt * B_c\n",
    "\n",
    "# Solve DARE for LQR gain\n",
    "P_lqr = solve_discrete_are(A_d, B_d, Q_lqr, R_lqr)\n",
    "K_lqr = np.linalg.solve(R_lqr + B_d.T @ P_lqr @ B_d, B_d.T @ P_lqr @ A_d)\n",
    "\n",
    "print(\"LQR gain K:\")\n",
    "print(K_lqr)\n",
    "print(f\"\\nGain norm: {np.linalg.norm(K_lqr):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqr-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lqr_ff_control(x, x_ref, u_ref, K, u_max):\n",
    "    \"\"\"\n",
    "    LQR + Feedforward control.\n",
    "    \n",
    "    u = u_ref - K @ (x - x_ref)\n",
    "    \"\"\"\n",
    "    e = state_error(x, x_ref)\n",
    "    u = u_ref - K @ e\n",
    "    return np.clip(u, -u_max, u_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqr-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LQR+FF\n",
    "x_lqr = np.zeros((n_x, N))\n",
    "u_lqr = np.zeros((n_u, N))\n",
    "x_lqr[:, 0] = x_ref[:, 0]  # Start on reference\n",
    "\n",
    "rng_lqr = np.random.default_rng(42)\n",
    "final_waypoint = waypoints[-1]\n",
    "goal_tol = 0.1  # 10cm tolerance\n",
    "\n",
    "final_k_lqr = N - 1\n",
    "for k in range(N - 1):\n",
    "    # Control\n",
    "    u_lqr[:, k] = lqr_ff_control(x_lqr[:, k], x_ref[:, k], u_ref[:, k], K_lqr, u_max)\n",
    "    \n",
    "    # Propagate with noise\n",
    "    w = Q_noise_chol @ rng_lqr.standard_normal(n_x)\n",
    "    x_lqr[:, k+1] = nonlinear_dynamics(x_lqr[:, k], u_lqr[:, k], dt, L, tau) + w\n",
    "    \n",
    "    # Early termination when goal reached\n",
    "    dist_to_goal = np.sqrt((x_lqr[0, k+1] - final_waypoint[0])**2 + \n",
    "                           (x_lqr[1, k+1] - final_waypoint[1])**2)\n",
    "    if dist_to_goal < goal_tol and k > 50:\n",
    "        final_k_lqr = k + 1\n",
    "        break\n",
    "\n",
    "# Trim to actual length\n",
    "x_lqr = x_lqr[:, :final_k_lqr + 1]\n",
    "u_lqr = u_lqr[:, :final_k_lqr + 1]\n",
    "t_lqr = t[:final_k_lqr + 1]\n",
    "x_ref_lqr = x_ref[:, :final_k_lqr + 1]\n",
    "\n",
    "# Compute tracking error\n",
    "err_lqr = np.sqrt((x_lqr[0, :] - x_ref_lqr[0, :])**2 + (x_lqr[1, :] - x_ref_lqr[1, :])**2)\n",
    "rms_lqr = np.sqrt(np.mean(err_lqr**2))\n",
    "\n",
    "print(f\"LQR+FF: {final_k_lqr + 1} steps ({(final_k_lqr + 1)*dt:.1f}s)\")\n",
    "print(f\"LQR+FF tracking RMS error: {rms_lqr*100:.2f} cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mpc-heading",
   "metadata": {},
   "source": [
    "## Controller 2: QP-MPC (OSQP)\n",
    "\n",
    "Model Predictive Control solves an optimization problem at each timestep:\n",
    "\n",
    "$$\\min_{\\delta\\mathbf{u}_{0:N-1}} \\sum_{k=0}^{N} \\mathbf{e}_k^T Q \\mathbf{e}_k + \\sum_{k=0}^{N-1} \\delta\\mathbf{u}_k^T R \\delta\\mathbf{u}_k + \\sum_{k=0}^{N-1} \\Delta\\mathbf{u}_k^T R_{rate} \\Delta\\mathbf{u}_k$$\n",
    "\n",
    "subject to:\n",
    "- Error dynamics: $\\mathbf{e}_{k+1} = A \\mathbf{e}_k + B \\delta\\mathbf{u}_k$\n",
    "- Input bounds: $|\\mathbf{u}_{ref} + \\delta\\mathbf{u}| \\leq u_{max}$\n",
    "\n",
    "### Rate Penalty\n",
    "\n",
    "The $R_{rate}$ term penalizes $\\Delta\\mathbf{u} = \\mathbf{u}_k - \\mathbf{u}_{k-1}$. This is crucial for systems with motor dynamics — rapid control changes cannot be tracked by slow actuators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qpmpc-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QP-MPC parameters\n",
    "Q_mpc = np.diag([10.0, 10.0, 5.0, 0.1, 0.1])\n",
    "R_mpc = np.diag([0.1, 0.1])\n",
    "R_rate = np.diag([2.0, 2.0])  # Rate penalty for smooth control\n",
    "N_horizon = 15  # 0.75s at 20Hz\n",
    "\n",
    "mpc_qp = QPMPC(L, tau, dt, Q_mpc, R_mpc, R_rate, N_horizon, u_max, exp_weight=1.0)\n",
    "\n",
    "print(f\"QP-MPC initialized:\")\n",
    "print(f\"  Horizon: {N_horizon} steps ({N_horizon*dt:.2f}s)\")\n",
    "print(f\"  Rate penalty: {R_rate[0,0]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qpmpc-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate QP-MPC\n",
    "x_qpmpc = np.zeros((n_x, N))\n",
    "u_qpmpc = np.zeros((n_u, N))\n",
    "x_qpmpc[:, 0] = x_ref[:, 0]\n",
    "\n",
    "rng_qpmpc = np.random.default_rng(42)\n",
    "mpc_qp.reset()  # Clear warm start\n",
    "\n",
    "solve_times = []\n",
    "\n",
    "import time\n",
    "print(\"Running QP-MPC simulation...\")\n",
    "\n",
    "final_k_qpmpc = N - 1\n",
    "for k in range(N - 1):\n",
    "    # Get reference horizon\n",
    "    k_end = min(k + N_horizon + 1, N)\n",
    "    x_ref_h = x_ref[:, k:k_end]\n",
    "    u_ref_h = u_ref[:, k:k_end]\n",
    "    \n",
    "    # Pad if needed\n",
    "    if x_ref_h.shape[1] < N_horizon + 1:\n",
    "        pad = N_horizon + 1 - x_ref_h.shape[1]\n",
    "        x_ref_h = np.hstack([x_ref_h, np.tile(x_ref[:, -1:], (1, pad))])\n",
    "        u_ref_h = np.hstack([u_ref_h, np.tile(u_ref[:, -1:], (1, pad))])\n",
    "    \n",
    "    # Control\n",
    "    t0 = time.perf_counter()\n",
    "    u_qpmpc[:, k] = mpc_qp.compute_control(x_qpmpc[:, k], x_ref_h, u_ref_h)\n",
    "    solve_times.append(time.perf_counter() - t0)\n",
    "    \n",
    "    # Propagate\n",
    "    w = Q_noise_chol @ rng_qpmpc.standard_normal(n_x)\n",
    "    x_qpmpc[:, k+1] = nonlinear_dynamics(x_qpmpc[:, k], u_qpmpc[:, k], dt, L, tau) + w\n",
    "    \n",
    "    # Early termination when goal reached\n",
    "    dist_to_goal = np.sqrt((x_qpmpc[0, k+1] - final_waypoint[0])**2 + \n",
    "                           (x_qpmpc[1, k+1] - final_waypoint[1])**2)\n",
    "    if dist_to_goal < goal_tol and k > 50:\n",
    "        final_k_qpmpc = k + 1\n",
    "        break\n",
    "\n",
    "# Trim to actual length\n",
    "x_qpmpc = x_qpmpc[:, :final_k_qpmpc + 1]\n",
    "u_qpmpc = u_qpmpc[:, :final_k_qpmpc + 1]\n",
    "t_qpmpc = t[:final_k_qpmpc + 1]\n",
    "x_ref_qpmpc = x_ref[:, :final_k_qpmpc + 1]\n",
    "\n",
    "err_qpmpc = np.sqrt((x_qpmpc[0, :] - x_ref_qpmpc[0, :])**2 + (x_qpmpc[1, :] - x_ref_qpmpc[1, :])**2)\n",
    "rms_qpmpc = np.sqrt(np.mean(err_qpmpc**2))\n",
    "\n",
    "print(f\"\\nQP-MPC: {final_k_qpmpc + 1} steps ({(final_k_qpmpc + 1)*dt:.1f}s)\")\n",
    "print(f\"QP-MPC tracking RMS error: {rms_qpmpc*100:.2f} cm\")\n",
    "print(f\"Solve time: {np.mean(solve_times)*1000:.2f} ms avg, {np.max(solve_times)*1000:.2f} ms max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nlmpc-heading",
   "metadata": {},
   "source": [
    "## Controller 3: Nonlinear MPC\n",
    "\n",
    "Nonlinear MPC uses the full nonlinear dynamics for prediction:\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = f(\\mathbf{x}_k, \\mathbf{u}_k)$$\n",
    "\n",
    "This makes the optimization **non-convex** — it cannot be formulated as a QP. The trade-offs vs QP-MPC:\n",
    "\n",
    "| Aspect | QP-MPC | NL-MPC |\n",
    "|--------|--------|--------|\n",
    "| Dynamics | Linearized | Full nonlinear |\n",
    "| Problem | Convex QP | Non-convex NLP |\n",
    "| Solve time | ~2 ms | ~20 ms |\n",
    "| Convergence | Guaranteed | May find local min |\n",
    "| Embedded | Yes (OSQP) | Difficult |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nlmpc-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NL-MPC parameters\n",
    "Q_nl = np.diag([10.0, 10.0, 5.0, 0.1, 0.1])\n",
    "R_nl = np.diag([0.1, 0.1])\n",
    "N_horizon_nl = 10\n",
    "N_control_nl = 5  # Control horizon (reduces optimization variables)\n",
    "\n",
    "# Create dynamics function for NL-MPC\n",
    "# The MPC accepts a generic f(x, u, dt) -> x_next interface\n",
    "def dynamics_for_mpc(x, u, dt_step):\n",
    "    return nonlinear_dynamics(x, u, dt_step, L, tau)\n",
    "\n",
    "mpc_nl = NonlinearMPC(dynamics_for_mpc, dt, Q_nl, R_nl, N_horizon_nl, N_control_nl, u_max)\n",
    "\n",
    "print(f\"NL-MPC initialized:\")\n",
    "print(f\"  Prediction horizon: {N_horizon_nl} steps ({N_horizon_nl*dt:.2f}s)\")\n",
    "print(f\"  Control horizon: {N_control_nl} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nlmpc-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate NL-MPC\n",
    "x_nlmpc = np.zeros((n_x, N))\n",
    "u_nlmpc = np.zeros((n_u, N))\n",
    "x_nlmpc[:, 0] = x_ref[:, 0]\n",
    "\n",
    "rng_nlmpc = np.random.default_rng(42)\n",
    "mpc_nl.reset()\n",
    "\n",
    "solve_times_nl = []\n",
    "\n",
    "print(\"Running NL-MPC simulation (slower)...\")\n",
    "\n",
    "final_k_nlmpc = N - 1\n",
    "for k in range(N - 1):\n",
    "    # Get reference horizon\n",
    "    k_end = min(k + N_horizon_nl + 1, N)\n",
    "    x_ref_h = x_ref[:, k:k_end]\n",
    "    u_ref_h = u_ref[:, k:k_end]\n",
    "    \n",
    "    # Pad if needed\n",
    "    if x_ref_h.shape[1] < N_horizon_nl + 1:\n",
    "        pad = N_horizon_nl + 1 - x_ref_h.shape[1]\n",
    "        x_ref_h = np.hstack([x_ref_h, np.tile(x_ref[:, -1:], (1, pad))])\n",
    "        u_ref_h = np.hstack([u_ref_h, np.tile(u_ref[:, -1:], (1, pad))])\n",
    "    \n",
    "    # Control\n",
    "    t0 = time.perf_counter()\n",
    "    u_nlmpc[:, k] = mpc_nl.compute_control(x_nlmpc[:, k], x_ref_h, u_ref_h)\n",
    "    solve_times_nl.append(time.perf_counter() - t0)\n",
    "    \n",
    "    # Propagate\n",
    "    w = Q_noise_chol @ rng_nlmpc.standard_normal(n_x)\n",
    "    x_nlmpc[:, k+1] = nonlinear_dynamics(x_nlmpc[:, k], u_nlmpc[:, k], dt, L, tau) + w\n",
    "    \n",
    "    # Early termination when goal reached\n",
    "    dist_to_goal = np.sqrt((x_nlmpc[0, k+1] - final_waypoint[0])**2 + \n",
    "                           (x_nlmpc[1, k+1] - final_waypoint[1])**2)\n",
    "    if dist_to_goal < goal_tol and k > 50:\n",
    "        final_k_nlmpc = k + 1\n",
    "        break\n",
    "\n",
    "# Trim to actual length\n",
    "x_nlmpc = x_nlmpc[:, :final_k_nlmpc + 1]\n",
    "u_nlmpc = u_nlmpc[:, :final_k_nlmpc + 1]\n",
    "t_nlmpc = t[:final_k_nlmpc + 1]\n",
    "x_ref_nlmpc = x_ref[:, :final_k_nlmpc + 1]\n",
    "\n",
    "err_nlmpc = np.sqrt((x_nlmpc[0, :] - x_ref_nlmpc[0, :])**2 + (x_nlmpc[1, :] - x_ref_nlmpc[1, :])**2)\n",
    "rms_nlmpc = np.sqrt(np.mean(err_nlmpc**2))\n",
    "\n",
    "print(f\"\\nNL-MPC: {final_k_nlmpc + 1} steps ({(final_k_nlmpc + 1)*dt:.1f}s)\")\n",
    "print(f\"NL-MPC tracking RMS error: {rms_nlmpc*100:.2f} cm\")\n",
    "print(f\"Solve time: {np.mean(solve_times_nl)*1000:.2f} ms avg, {np.max(solve_times_nl)*1000:.2f} ms max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-heading",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Trajectory comparison\n",
    "ax = axes[0, 0]\n",
    "ax.plot(x_ref[0, :], x_ref[1, :], 'k--', linewidth=2, label='Reference', alpha=0.5)\n",
    "ax.plot(x_lqr[0, :], x_lqr[1, :], 'b-', linewidth=1.5, label='LQR+FF')\n",
    "ax.plot(x_qpmpc[0, :], x_qpmpc[1, :], 'r-', linewidth=1.5, label='QP-MPC')\n",
    "ax.plot(x_nlmpc[0, :], x_nlmpc[1, :], 'g-', linewidth=1.5, label='NL-MPC')\n",
    "ax.plot(waypoints[:, 0], waypoints[:, 1], 'ko', markersize=8)\n",
    "ax.set_xlabel('x [m]')\n",
    "ax.set_ylabel('y [m]')\n",
    "ax.axis('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('Trajectory Comparison')\n",
    "\n",
    "# Tracking error (use individual time arrays)\n",
    "ax = axes[0, 1]\n",
    "ax.plot(t_lqr, err_lqr*100, 'b-', linewidth=1.5, label=f'LQR+FF ({rms_lqr*100:.1f}cm)')\n",
    "ax.plot(t_qpmpc, err_qpmpc*100, 'r-', linewidth=1.5, label=f'QP-MPC ({rms_qpmpc*100:.1f}cm)')\n",
    "ax.plot(t_nlmpc, err_nlmpc*100, 'g-', linewidth=1.5, label=f'NL-MPC ({rms_nlmpc*100:.1f}cm)')\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Position Error [cm]')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('Tracking Error (RMS in legend)')\n",
    "\n",
    "# Control inputs (left wheel)\n",
    "ax = axes[1, 0]\n",
    "ax.plot(t_lqr[:-1], u_lqr[0, :-1], 'b-', linewidth=1, label='LQR+FF', alpha=0.7)\n",
    "ax.plot(t_qpmpc[:-1], u_qpmpc[0, :-1], 'r-', linewidth=1, label='QP-MPC', alpha=0.7)\n",
    "ax.plot(t_nlmpc[:-1], u_nlmpc[0, :-1], 'g-', linewidth=1, label='NL-MPC', alpha=0.7)\n",
    "ax.plot(t, u_ref[0, :], 'k--', linewidth=1, label='Feedforward', alpha=0.5)\n",
    "ax.axhline(u_max, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(-u_max, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('$u_l$ [m/s]')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('Left Wheel Command')\n",
    "\n",
    "# Solve time histogram\n",
    "ax = axes[1, 1]\n",
    "ax.hist(np.array(solve_times)*1000, bins=30, alpha=0.7, color='red', label=f'QP-MPC ({np.mean(solve_times)*1000:.1f}ms)')\n",
    "ax.hist(np.array(solve_times_nl)*1000, bins=30, alpha=0.7, color='green', label=f'NL-MPC ({np.mean(solve_times_nl)*1000:.1f}ms)')\n",
    "ax.axvline(dt*1000, color='k', linestyle='-', linewidth=2, label=f'Control period ({dt*1000:.0f}ms)')\n",
    "ax.set_xlabel('Solve Time [ms]')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('MPC Solve Time Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTROLLER COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Controller':<15} {'RMS Error':<12} {'Solve Time':<15} {'Embedded?':<12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'LQR+FF':<15} {rms_lqr*100:>6.2f} cm    {'< 0.1 ms':<15} {'Yes':<12}\")\n",
    "print(f\"{'QP-MPC':<15} {rms_qpmpc*100:>6.2f} cm    {f'{np.mean(solve_times)*1000:.1f} ms':<15} {'Yes (OSQP)':<12}\")\n",
    "print(f\"{'NL-MPC':<15} {rms_nlmpc*100:>6.2f} cm    {f'{np.mean(solve_times_nl)*1000:.1f} ms':<15} {'Difficult':<12}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guidelines-heading",
   "metadata": {},
   "source": [
    "## Tuning Guidelines\n",
    "\n",
    "### LQR+FF\n",
    "\n",
    "- **Q diagonal**: Higher values increase responsiveness to that state error\n",
    "- **R diagonal**: Higher values reduce control effort (smoother but slower)\n",
    "- Linearization point affects performance — choose representative operating conditions\n",
    "\n",
    "### QP-MPC\n",
    "\n",
    "- **N_horizon**: Longer horizons improve anticipation but increase computation\n",
    "- **R_rate**: Critical for motor dynamics — higher = smoother control = better tracking with slow actuators\n",
    "- **exp_weight**: >1.0 emphasizes end-of-horizon accuracy\n",
    "\n",
    "### NL-MPC\n",
    "\n",
    "- **N_control**: Reducing this cuts optimization variables (faster) but limits flexibility\n",
    "- Use when QP-MPC linearization error is unacceptable (large heading changes)\n",
    "\n",
    "### General\n",
    "\n",
    "- Always include feedforward — pure feedback will always lag\n",
    "- Ensure reference trajectory is physically realizable\n",
    "- Test with realistic noise to validate robustness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
